# ğŸ“„ PDF Research Agent

A multi-agent research assistant built using **LangChain**, **Streamlit**, and **Tavily API** that analyzes research paper PDFs. This tool extracts metadata, verifies authorship, estimates impact, and benchmarks the paper's novelty by comparing it with similar papers online.

---

## ğŸš€ Features

### ğŸ§¾ Metadata Extractor
Extracts:
- Title, Abstract, Keywords
- Authors and Authorship positions
- Journal/Conference name, DOI, Year

### âœï¸ Authorship Verifier
Evaluates:
- Contribution quality
- Author position and significance
- Provides a verdict and checklist-style feedback

### ğŸ“ˆ Impact Estimator
Estimates:
- Technical depth and innovation
- Practical and academic value

### ğŸ“Š Research Benchmarking (RAG)
Benchmarks:
- Originality and relevance
- Compares your abstract with top 3 similar research papers
- Provides a 2â€“3 line summary and % similarity insight

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/pdf-research-agent.git
cd pdf-research-agent

2. Create a Virtual Environment
bash
Copy
Edit
python3 -m venv .venv
source .venv/bin/activate
3. Install Dependencies
bash
Copy
Edit
pip install -r requirements.txt
4. Set Up Environment Variables
Create a .env file in the root directory:

env
Copy
Edit
TAVILY_API_KEY=your_tavily_api_key_here
Ensure llm_config.py contains a valid LLM configuration (Google Gemini, OpenAI, etc.) exposed as llm.

â–¶ï¸ Run the App
bash
Copy
Edit
streamlit run app.py
ğŸ“ Project Structure
bash
Copy
Edit
.
â”œâ”€â”€ app.py
â”œâ”€â”€ llm_config.py
â”œâ”€â”€ .env
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ metadata_extractor.py
â”‚   â”œâ”€â”€ authorship_verifier.py
â”‚   â”œâ”€â”€ impact_estimator.py
â”‚   â””â”€â”€ benchmarking_agent.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ pdf_utils.py
â”‚   â””â”€â”€ parser_utils.py
â””â”€â”€ requirements.txt
ğŸ“Œ Requirements
Python 3.8+

Streamlit

Tavily SDK

LangChain-compatible LLM (OpenAI, Gemini, etc.)

